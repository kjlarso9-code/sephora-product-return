# ======================================================================
# Hands-On 3 ‚Äî CLASSIFICATION (Sephora Product Return Risk)
# Neural Network (MLPClassifier) + SVC (compact sweeps, fast)
# Logs to Databricks MLflow (fallback to local if creds not present)
# Metrics: Accuracy, F1, ROC-AUC
# User: <kjlarso9>@asu.edu
# ======================================================================
scikit-learn
pandas
matplotlib
numpy
mlflow
databricks-sdk

# ---------- MLflow setup ----------
import os, mlflow, mlflow.sklearn
from databricks.sdk import WorkspaceClient

# ===================================================================================
# Databricks MLflow (use Colab Secrets)
# ===================================================================================
databricks_host = None
databricks_token = None
try:
    from google.colab import userdata
    databricks_host = userdata.get("DATABRICKS_HOST")
    databricks_token = userdata.get("DATABRICKS_TOKEN")
except ImportError:
    print("google.colab.userdata not available. Ensure you are running in Colab or set env vars manually.")
except Exception as e:
    print(f"An unexpected error occurred while trying to get Colab secrets: {e}")

if databricks_host and databricks_token:
    os.environ["DATABRICKS_HOST"]  = databricks_host
    os.environ["DATABRICKS_TOKEN"] = databricks_token
else:
    print("Warning: DATABRICKS_HOST or DATABRICKS_TOKEN not found in Colab secrets or environment variables.")
    print("MLflow tracking to Databricks may fail. Please configure these secrets.")

mlflow.set_tracking_uri("databricks")

# !!! IMPORTANT: Change experiment name if you want a fresh one
experiment_name = "/Users/kjlarso9@asu.edu/SephoraProductReturnRisk_Classification"

mlflow.set_experiment(experiment_name)
print("Tracking URI:", mlflow.get_tracking_uri())
print("Experiment:", experiment_name)

# CHANGE YOUR EMAIL HERE TOO (already your email)
USER_EMAIL = "kjlarso9@asu.edu"
EXP_NAME   = "SephoraProductReturnRisk_Classification"

# ---------- Imports ----------
import io, json
import numpy as np, pandas as pd, matplotlib.pyplot as plt
from google.colab import files

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import (
    accuracy_score,
    f1_score,
    roc_auc_score,
    classification_report,
    confusion_matrix
)
from sklearn.impute import SimpleImputer
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
from sklearn.inspection import permutation_importance

plt.rcParams["figure.dpi"] = 120
mlflow.sklearn.autolog(log_models=True)

# ===================================================================================
# Load Sephora CSV ‚Äî Upload when prompted
# ===================================================================================
print("üìÅ Please choose your Sephora CSV (e.g., 'sephora_website_dataset.csv').")
uploaded = files.upload()
if len(uploaded) == 0:
    raise RuntimeError("No file uploaded. Please upload the Sephora CSV.")

CSV_PATH = list(uploaded.keys())[0]
print(f"‚úÖ Received file: {CSV_PATH}")

def robust_read_csv_from_upload(fname: str) -> pd.DataFrame:
    """Reads uploaded CSV with auto-detected separator; handles semicolons; cleans headers."""
    try:
        df = pd.read_csv(io.BytesIO(uploaded[fname]), sep=None, engine="python")
    except Exception:
        df = pd.read_csv(io.BytesIO(uploaded[fname]))
    if df.shape[1] == 1 and ";" in df.columns[0]:
        df = pd.read_csv(io.BytesIO(uploaded[fname]), sep=";", engine="python")
    df.columns = [c.strip().strip('"') for c in df.columns]
    return df

print("‚¨ÜÔ∏è Loading Sephora CSV...")
df = robust_read_csv_from_upload(CSV_PATH)
print("Shape:", df.shape)
print("Columns:", list(df.columns))

# ===================================================================================
# Target Engineering: "High return / dislike risk"
#    - Use 'rating' column
#    - Drop rows with missing or 0 rating (0 often means 'unrated')
#    - high_risk = 1 if rating < 3, else 0
# ===================================================================================
if "rating" not in df.columns:
    raise ValueError("Expected a 'rating' column in the Sephora dataset.")

df = df.copy()
df = df[~df["rating"].isna()]         # drop missing ratings
df = df[df["rating"] > 0]             # drop '0' ratings (unrated / noise)

df["high_return_risk"] = (df["rating"] < 3.0).astype(int)
print(df["high_return_risk"].value_counts())

TARGET = "high_return_risk"

# ===================================================================================
# Feature selection
# We'll drop:
#   - id: arbitrary identifier
#   - rating: since we used it to make the label
#   - URL: just a link
#   - long text blobs (details, how_to_use, MarketingFlags_content) to keep it faster
# You *could* keep ingredients/text for a richer (but heavier) model.
# ===================================================================================
drop_cols = [
    "id",
    "rating",
    "URL",
    "details",
    "how_to_use",
    "MarketingFlags_content"
]
drop_cols = [c for c in drop_cols if c in df.columns]

y = df[TARGET]
X = df.drop(columns=[TARGET] + drop_cols)

# Cast numerics to float64 (prevents MLflow schema warnings)
for c in X.columns:
    if pd.api.types.is_numeric_dtype(X[c]):
        X[c] = pd.to_numeric(X[c], errors="coerce").astype("float64")

num_cols = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]
cat_cols = [c for c in X.columns if c not in num_cols]
print(f"Numeric cols: {len(num_cols)} -> {num_cols}")
print(f"Categorical cols: {len(cat_cols)} -> {cat_cols}")

# Preprocessor
prep = ColumnTransformer([
    ("num", Pipeline([("imputer", SimpleImputer(strategy="mean")), ("scaler", StandardScaler())]), num_cols),
    ("cat", OneHotEncoder(handle_unknown="ignore"), cat_cols),
])

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# ===================================================================================
# Helpers
# ===================================================================================
def log_confusion_matrix_plot(y_true, y_pred, fname="confusion_matrix.png"):
    cm = confusion_matrix(y_true, y_pred)
    fig, ax = plt.subplots(figsize=(4,4))
    im = ax.imshow(cm)
    ax.set_title("Confusion Matrix")
    ax.set_xlabel("Predicted")
    ax.set_ylabel("Actual")
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, cm[i, j], ha="center", va="center")
    plt.tight_layout()
    fig.savefig(fname); plt.close(fig)
    mlflow.log_artifact(fname)

def log_prob_hist_plot(y_prob, fname="prob_hist.png"):
    fig, ax = plt.subplots(figsize=(5,4))
    ax.hist(y_prob, bins=20)
    ax.set_title("Predicted High-Risk Probability")
    ax.set_xlabel("P(high_return_risk=1)")
    ax.set_ylabel("Count")
    plt.tight_layout()
    fig.savefig(fname); plt.close(fig)
    mlflow.log_artifact(fname)

def feat_names_from_preprocessor(prep, num_cols, cat_cols):
    names = list(num_cols)
    if len(cat_cols):
        ohe = prep.named_transformers_["cat"]
        names += ohe.get_feature_names_out(cat_cols).tolist()
    return names

def perm_top_for_best(pipe, X_eval, y_eval, names, family_prefix, n_repeats=3, max_rows=4000):
    """Fast permutation importance on a subsample; logs top-20 text artifact."""
    if len(X_eval) > max_rows:
        rs = np.random.RandomState(42)
        idx = rs.choice(len(X_eval), size=max_rows, replace=False)
        X_eval, y_eval = X_eval.iloc[idx], y_eval.iloc[idx]
    perm = permutation_importance(
        pipe,
        X_eval,
        y_eval,
        n_repeats=n_repeats,
        random_state=42,
        scoring="f1"  # classification setting
    )
    imps = perm.importances_mean
    top = int(np.argmax(imps))
    top_name, top_val = names[top], float(imps[top])
    order = np.argsort(imps)[::-1]
    lines = ["rank,feature,importance_mean"]
    for r, j in enumerate(order[:20], start=1):
        lines.append(f"{r},{names[j]},{imps[j]:.8f}")
    with open(f"{family_prefix}_perm_top20.txt","w") as f:
        f.write("\n".join(lines))
    mlflow.log_artifact(f"{family_prefix}_perm_top20.txt")
    return top_name, top_val

# ===================================================================================
# 1) Neural Network Classifier (compact sweep; fast)
# ===================================================================================
nn_grid = [
    {"hidden_layer_sizes": (64,),   "learning_rate_init": 1e-3, "alpha": 1e-4},
    {"hidden_layer_sizes": (64,32), "learning_rate_init": 5e-4, "alpha": 1e-4},
    {"hidden_layer_sizes": (64,32), "learning_rate_init": 5e-4, "alpha": 1e-3},
]
best_nn = {"f1": -np.inf, "run_id": None, "pipe": None}

for p in nn_grid:
    name = f"NNCls_h{p['hidden_layer_sizes']}_lr{p['learning_rate_init']}_alpha{p['alpha']}"
    with mlflow.start_run(run_name=name) as run:
        model = MLPClassifier(
            hidden_layer_sizes=p["hidden_layer_sizes"],
            learning_rate_init=p["learning_rate_init"],
            alpha=p["alpha"],
            max_iter=250,
            early_stopping=True,
            n_iter_no_change=10,
            random_state=42
        )
        pipe = Pipeline([("prep", prep), ("clf", model)])
        pipe.fit(X_train, y_train)

        y_pred = pipe.predict(X_test)
        y_proba = pipe.predict_proba(X_test)[:, 1]

        acc = float(accuracy_score(y_test, y_pred))
        f1  = float(f1_score(y_test, y_pred))
        roc = float(roc_auc_score(y_test, y_proba))

        mlflow.log_metric("accuracy", acc)
        mlflow.log_metric("f1", f1)
        mlflow.log_metric("roc_auc", roc)

        # Tags to make Databricks table usage obvious
        mlflow.set_tag("primary_metric", "f1")
        mlflow.set_tag("primary_metric_goal", "maximize")
        mlflow.set_tag("model_family", "MLPClassifier")

        # Tiny artifact with the metrics + classification report
        with open("run_metrics.json", "w") as f:
            json.dump({"accuracy": acc, "f1": f1, "roc_auc": roc}, f, indent=2)
        mlflow.log_artifact("run_metrics.json")

        with open("classification_report.txt", "w") as f:
            f.write(classification_report(y_test, y_pred))
        mlflow.log_artifact("classification_report.txt")

        log_confusion_matrix_plot(y_test, y_pred, "nn_confusion.png")
        log_prob_hist_plot(y_proba, "nn_prob_hist.png")

        if f1 > best_nn["f1"]:
            best_nn.update({"f1": f1, "run_id": run.info.run_id, "pipe": pipe})

# Permutation importance ONLY for best NN (keeps runtime short)
if best_nn["run_id"]:
    names = feat_names_from_preprocessor(prep, num_cols, cat_cols)
    with mlflow.start_run(run_id=best_nn["run_id"]):
        top, val = perm_top_for_best(best_nn["pipe"], X_test, y_test, names, "nn")
        mlflow.log_param("most_informative_feature", top)
        mlflow.log_metric("most_informative_importance", val)

# ===================================================================================
# 2) Support Vector Classification (compact sweep; fast)
# ===================================================================================
svr_grid = [
    {"kernel": "linear", "C": 1,  "epsilon": 0.1, "gamma": "scale"},
    {"kernel": "linear", "C": 10, "epsilon": 0.1, "gamma": "scale"},
    {"kernel": "rbf",    "C": 1,  "epsilon": 0.1, "gamma": "scale"},
    {"kernel": "rbf",    "C": 10, "epsilon": 0.1, "gamma": "scale"},
]

# We‚Äôll ignore epsilon (it‚Äôs a regression param) but keep structure similar
svc_grid = [
    {"kernel": "linear", "C": 1,  "gamma": "scale"},
    {"kernel": "linear", "C": 10, "gamma": "scale"},
    {"kernel": "rbf",    "C": 1,  "gamma": "scale"},
    {"kernel": "rbf",    "C": 10, "gamma": "scale"},
]

best_svc = {"f1": -np.inf, "run_id": None, "pipe": None}

for p in svc_grid:
    name = f"SVC_k{p['kernel']}_C{p['C']}_g{p['gamma']}"
    with mlflow.start_run(run_name=name) as run:
        model = SVC(
            kernel=p['kernel'],
            C=p['C'],
            gamma=p['gamma'],
            probability=True,
            cache_size=500
        )
        pipe = Pipeline([("prep", prep), ("clf", model)])
        pipe.fit(X_train, y_train)

        y_pred = pipe.predict(X_test)
        y_proba = pipe.predict_proba(X_test)[:, 1]

        acc = float(accuracy_score(y_test, y_pred))
        f1  = float(f1_score(y_test, y_pred))
        roc = float(roc_auc_score(y_test, y_proba))

        mlflow.log_metric("accuracy", acc)
        mlflow.log_metric("f1", f1)
        mlflow.log_metric("roc_auc", roc)

        # Tags to make Databricks table usage obvious
        mlflow.set_tag("primary_metric", "f1")
        mlflow.set_tag("primary_metric_goal", "maximize")
        mlflow.set_tag("model_family", "SVC")

        with open("run_metrics.json", "w") as f:
            json.dump({"accuracy": acc, "f1": f1, "roc_auc": roc}, f, indent=2)
        mlflow.log_artifact("run_metrics.json")

        with open("classification_report.txt", "w") as f:
            f.write(classification_report(y_test, y_pred))
        mlflow.log_artifact("classification_report.txt")

        log_confusion_matrix_plot(y_test, y_pred, "svc_confusion.png")
        log_prob_hist_plot(y_proba, "svc_prob_hist.png")

        if f1 > best_svc["f1"]:
            best_svc.update({"f1": f1, "run_id": run.info.run_id, "pipe": pipe})

# Permutation importance ONLY for best SVC (keeps runtime short)
if best_svc["run_id"]:
    names = feat_names_from_preprocessor(prep, num_cols, cat_cols)
    with mlflow.start_run(run_id=best_svc["run_id"]):
        top, val = perm_top_for_best(best_svc["pipe"], X_test, y_test, names, "svc")
        mlflow.log_param("most_informative_feature", top)
        mlflow.log_metric("most_informative_importance", val)

# ===========================
# Databricks runs table helper
# ===========================
from urllib.parse import quote

host = (os.environ.get("DATABRICKS_HOST") or "").rstrip("/")
exp = mlflow.get_experiment_by_name(f"/Users/{USER_EMAIL}/{EXP_NAME}")
if exp and host:
    exp_url = f"{host}/#mlflow/experiments/{quote(exp.experiment_id)}"
    print("üîó Open your experiment:", exp_url)

print("\nüß≠ In the Databricks Experiments UI:")
print("   ‚Ä¢ Click 'Add column' ‚Üí metrics: accuracy, f1, roc_auc")
print("   ‚Ä¢ Sort by f1 (descending) to see the best runs")
print("   ‚Ä¢ You can also filter by tag 'model_family' (MLPClassifier / SVC)")

print("\n‚úÖ Hands-On 3 (Classification): all runs logged.")
